{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-20T02:51:12.141472200Z",
     "start_time": "2023-05-20T02:51:05.170382300Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from groundingdino.util.inference import load_model, load_image, annotate, predict as _predict\n",
    "import cv2\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\transformers\\modeling_utils.py:433: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  with safe_open(checkpoint_file, framework=\"pt\") as f:\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"GroundingDINO_SwinT_OGC.py\", \"groundingdino_swint_ogc.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T02:51:14.661699Z",
     "start_time": "2023-05-20T02:51:12.142472900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def predict(image, caption, box_threshold, text_threshold, save=False):\n",
    "    fname = image.split(os.sep)[-1]\n",
    "    image_source, image = load_image(image)\n",
    "    boxes, confidence, phrases = _predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=caption,\n",
    "    box_threshold=box_threshold,\n",
    "    text_threshold=text_threshold)\n",
    "    if save:\n",
    "        os.makedirs('output', exist_ok=True)\n",
    "        annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=confidence, phrases=phrases)\n",
    "        cv2.imwrite(os.path.join('output', fname), annotated_frame)\n",
    "    boxes = torchvision.ops.box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").tolist()\n",
    "    confidence = confidence.tolist()\n",
    "    preds = [{\n",
    "        'Image_ID': fname.split('.')[0],\n",
    "        'class': 0,  # TODO: replace with actual ReID model results, but for now can just submit bbox and see map, 1: suspect, 0: not suspect\n",
    "        'confidence': confidence[i],\n",
    "        'ymin': boxes[i][1],\n",
    "        'xmin': boxes[i][0],\n",
    "        'ymax': boxes[i][3],\n",
    "        'xmax': boxes[i][2]\n",
    "    } for i in range(len(boxes))]\n",
    "    preds.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    return preds[:4]  # each image only allow 4 boxes at most so take the most confident 4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T02:51:14.676699300Z",
     "start_time": "2023-05-20T02:51:14.664698800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1600 [00:00<?, ?it/s]C:\\Program Files\\Python39\\lib\\site-packages\\transformers\\modeling_utils.py:866: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "C:\\Program Files\\Python39\\lib\\site-packages\\torch\\utils\\checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "100%|██████████| 1600/1600 [04:46<00:00,  5.58it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_root = '../RT-DETR/dataset/test/images'\n",
    "TEXT_PROMPT = \"plushie\"\n",
    "BOX_TRESHOLD = 0.3\n",
    "TEXT_TRESHOLD = 0.3\n",
    "results = []\n",
    "\n",
    "for fname in tqdm.tqdm(os.listdir(dataset_root)):\n",
    "    results.extend(predict(os.path.join(dataset_root, fname), TEXT_PROMPT, BOX_TRESHOLD, TEXT_TRESHOLD, save=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T02:56:01.435621700Z",
     "start_time": "2023-05-20T02:51:14.678698800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('submission thresh0.3.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T03:01:11.050649700Z",
     "start_time": "2023-05-20T03:01:11.020637400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('submission thresh0.3.csv')\n",
    "df['Image_ID'] = df['Image_ID'].apply(lambda x: x.split('.')[0])\n",
    "df.to_csv('submission thresh0.3.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T03:01:24.142567300Z",
     "start_time": "2023-05-20T03:01:24.107445800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
