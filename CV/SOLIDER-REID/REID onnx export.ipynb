{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-29T05:33:45.163135700Z",
     "start_time": "2023-05-29T05:33:43.688229200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.make_model import make_model\n",
    "from config import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Transformer_type: swin_base_patch4_window7_224 as a backbone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alien\\Documents\\PyCharm-Projects\\TIL-2023\\CV\\SOLIDER-REID\\model\\backbones\\swin_transformer.py:1153: UserWarning: DeprecationWarning: pretrained is deprecated, please use \"init_cfg\" instead\n",
      "  warnings.warn('DeprecationWarning: pretrained is deprecated, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========building transformer===========\n"
     ]
    }
   ],
   "source": [
    "cfg.merge_from_file('TIL.yml')\n",
    "model = make_model(\n",
    "        cfg,\n",
    "        num_class=2,\n",
    "        camera_num=1,\n",
    "        view_num=1,\n",
    "        semantic_weight=cfg.MODEL.SEMANTIC_WEIGHT\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T05:33:45.467136500Z",
     "start_time": "2023-05-29T05:33:45.164138200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from log_SGD_continue_1e-4/transformer_29_map0.9159278553764464_acc0.5036038160324097.pth\n"
     ]
    }
   ],
   "source": [
    "model.load_param('log_SGD_continue_1e-4/transformer_29_map0.9159278553764464_acc0.5036038160324097.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T05:33:49.123901600Z",
     "start_time": "2023-05-29T05:33:48.026902700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "build_transformer(\n  (base): SwinTransformer(\n    (patch_embed): PatchEmbed(\n      (adap_padding): AdaptivePadding()\n      (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    )\n    (drop_after_pos): Dropout(p=0.0, inplace=False)\n    (stages): ModuleList(\n      (0): SwinBlockSequence(\n        (blocks): ModuleList(\n          (0-1): 2 x SwinBlock(\n            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (attn): ShiftWindowMSA(\n              (w_msa): WindowMSA(\n                (qkv): Linear(in_features=128, out_features=384, bias=True)\n                (attn_drop): Dropout(p=0.0, inplace=False)\n                (proj): Linear(in_features=128, out_features=128, bias=True)\n                (proj_drop): Dropout(p=0.0, inplace=False)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop): DropPath()\n            )\n            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (ffn): FFN(\n              (activate): GELU()\n              (layers): Sequential(\n                (0): Sequential(\n                  (0): Linear(in_features=128, out_features=512, bias=True)\n                  (1): GELU()\n                  (2): Dropout(p=0.0, inplace=False)\n                )\n                (1): Linear(in_features=512, out_features=128, bias=True)\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (dropout_layer): DropPath()\n            )\n          )\n        )\n        (downsample): PatchMerging(\n          (adap_padding): AdaptivePadding()\n          (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))\n          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (reduction): Linear(in_features=512, out_features=256, bias=False)\n        )\n      )\n      (1): SwinBlockSequence(\n        (blocks): ModuleList(\n          (0-1): 2 x SwinBlock(\n            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n            (attn): ShiftWindowMSA(\n              (w_msa): WindowMSA(\n                (qkv): Linear(in_features=256, out_features=768, bias=True)\n                (attn_drop): Dropout(p=0.0, inplace=False)\n                (proj): Linear(in_features=256, out_features=256, bias=True)\n                (proj_drop): Dropout(p=0.0, inplace=False)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop): DropPath()\n            )\n            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n            (ffn): FFN(\n              (activate): GELU()\n              (layers): Sequential(\n                (0): Sequential(\n                  (0): Linear(in_features=256, out_features=1024, bias=True)\n                  (1): GELU()\n                  (2): Dropout(p=0.0, inplace=False)\n                )\n                (1): Linear(in_features=1024, out_features=256, bias=True)\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (dropout_layer): DropPath()\n            )\n          )\n        )\n        (downsample): PatchMerging(\n          (adap_padding): AdaptivePadding()\n          (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))\n          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (reduction): Linear(in_features=1024, out_features=512, bias=False)\n        )\n      )\n      (2): SwinBlockSequence(\n        (blocks): ModuleList(\n          (0-17): 18 x SwinBlock(\n            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (attn): ShiftWindowMSA(\n              (w_msa): WindowMSA(\n                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n                (attn_drop): Dropout(p=0.0, inplace=False)\n                (proj): Linear(in_features=512, out_features=512, bias=True)\n                (proj_drop): Dropout(p=0.0, inplace=False)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop): DropPath()\n            )\n            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (ffn): FFN(\n              (activate): GELU()\n              (layers): Sequential(\n                (0): Sequential(\n                  (0): Linear(in_features=512, out_features=2048, bias=True)\n                  (1): GELU()\n                  (2): Dropout(p=0.0, inplace=False)\n                )\n                (1): Linear(in_features=2048, out_features=512, bias=True)\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (dropout_layer): DropPath()\n            )\n          )\n        )\n        (downsample): PatchMerging(\n          (adap_padding): AdaptivePadding()\n          (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))\n          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n        )\n      )\n      (3): SwinBlockSequence(\n        (blocks): ModuleList(\n          (0-1): 2 x SwinBlock(\n            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (attn): ShiftWindowMSA(\n              (w_msa): WindowMSA(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (attn_drop): Dropout(p=0.0, inplace=False)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (proj_drop): Dropout(p=0.0, inplace=False)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop): DropPath()\n            )\n            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (ffn): FFN(\n              (activate): GELU()\n              (layers): Sequential(\n                (0): Sequential(\n                  (0): Linear(in_features=1024, out_features=4096, bias=True)\n                  (1): GELU()\n                  (2): Dropout(p=0.0, inplace=False)\n                )\n                (1): Linear(in_features=4096, out_features=1024, bias=True)\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (dropout_layer): DropPath()\n            )\n          )\n        )\n      )\n    )\n    (norm0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (semantic_embed_w): ModuleList(\n      (0): Linear(in_features=2, out_features=256, bias=True)\n      (1): Linear(in_features=2, out_features=512, bias=True)\n      (2-3): 2 x Linear(in_features=2, out_features=1024, bias=True)\n    )\n    (semantic_embed_b): ModuleList(\n      (0): Linear(in_features=2, out_features=256, bias=True)\n      (1): Linear(in_features=2, out_features=512, bias=True)\n      (2-3): 2 x Linear(in_features=2, out_features=1024, bias=True)\n    )\n    (softplus): Softplus(beta=1, threshold=20)\n  )\n  (classifier): Linear(in_features=1024, out_features=2, bias=False)\n  (bottleneck): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout): Dropout(p=0.0, inplace=False)\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T05:33:49.782830500Z",
     "start_time": "2023-05-29T05:33:49.535646900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1024])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier = torch.nn.Identity()  # remove the classifier layer\n",
    "model(torch.randn(1, 3, 224, 224).cuda())[0].shape  # model has 2 outputs, global_featmaps, featmaps. We only use global_featmaps"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T05:33:51.560231200Z",
     "start_time": "2023-05-29T05:33:51.346230400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alien\\Documents\\PyCharm-Projects\\TIL-2023\\CV\\SOLIDER-REID\\model\\backbones\\swin_transformer.py:354: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  output_h = math.ceil(input_h / stride_h)\n",
      "C:\\Users\\alien\\Documents\\PyCharm-Projects\\TIL-2023\\CV\\SOLIDER-REID\\model\\backbones\\swin_transformer.py:355: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  output_w = math.ceil(input_w / stride_w)\n",
      "C:\\Users\\alien\\Documents\\PyCharm-Projects\\TIL-2023\\CV\\SOLIDER-REID\\model\\backbones\\swin_transformer.py:356: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  pad_h = max((output_h - 1) * stride_h +\n",
      "C:\\Users\\alien\\Documents\\PyCharm-Projects\\TIL-2023\\CV\\SOLIDER-REID\\model\\backbones\\swin_transformer.py:358: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  pad_w = max((output_w - 1) * stride_w +\n",
      "C:\\Users\\alien\\Documents\\PyCharm-Projects\\TIL-2023\\CV\\SOLIDER-REID\\model\\backbones\\swin_transformer.py:364: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pad_h > 0 or pad_w > 0:\n",
      "C:\\Users\\alien\\Documents\\PyCharm-Projects\\TIL-2023\\CV\\SOLIDER-REID\\model\\backbones\\swin_transformer.py:777: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert L == H * W, 'input feature has wrong size'\n",
      "C:\\Users\\alien\\Documents\\PyCharm-Projects\\TIL-2023\\CV\\SOLIDER-REID\\model\\backbones\\swin_transformer.py:860: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
      "C:\\Users\\alien\\Documents\\PyCharm-Projects\\TIL-2023\\CV\\SOLIDER-REID\\model\\backbones\\swin_transformer.py:842: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pad_r > 0 or pad_b:\n",
      "C:\\Users\\alien\\Documents\\PyCharm-Projects\\TIL-2023\\CV\\SOLIDER-REID\\model\\backbones\\swin_transformer.py:593: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert L == H * W, 'input feature has wrong size'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 224, 224).cuda()\n",
    "torch.onnx.export(model, x, 'REID.onnx',  # will produce many small files due to 2GB limit\n",
    "                  export_params=True,\n",
    "                  verbose=True,\n",
    "                  opset_version=18,  # TensorRT 8.6 supports up to 17, Torch 2.0.1 supports up to 18\n",
    "                  do_constant_folding=True,\n",
    "                  input_names = ['input'],  # with batch dim and batch dim is query + gallery images, query first. Always at least 2 image input (1 can run but no point cuz we need compare features)\n",
    "                  output_names = ['global_featmaps', 'featmaps'],  # the feature of all input images. For REID we only use the global featmap\n",
    "                  dynamic_axes={'input' : {0 : 'batchsize_num_images'}, 'global_featmaps' : {0 : 'batchsize_num_images'}, 'featmaps' : {0 : 'batchsize_num_images'}},  # dynamic batch size\n",
    "                  keep_initializers_as_inputs=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T05:34:32.931015900Z",
     "start_time": "2023-05-29T05:33:52.962986200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from config import cfg\n",
    "import torch\n",
    "\n",
    "cfg.merge_from_file('TIL.yml')\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "ort.get_available_providers()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T05:34:33.009016800Z",
     "start_time": "2023-05-29T05:34:32.933015900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "sess = ort.InferenceSession(\"REID.onnx\", sess_options=sess_options, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T05:34:39.011018300Z",
     "start_time": "2023-05-29T05:34:33.010017Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def load_img(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    # normalize with mean and std supplied in cfg\n",
    "    img = img / 255.0\n",
    "    for channel in range(3):\n",
    "        img[channel] -= cfg.INPUT.PIXEL_MEAN[channel]\n",
    "        img[channel] /= cfg.INPUT.PIXEL_STD[channel]\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "query_paths = [\"../RT-DETR/dataset/reid/test_old/query/image_0000.png\"]\n",
    "gallery_paths = [\"../RT-DETR/dataset/reid/test_old/bounding_box_test/-1_c0s1_1_1.png\", \"../RT-DETR/dataset/reid/test_old/bounding_box_test/-1_c0s1_1_2.png\"]\n",
    "imgs = [load_img(path) for path in query_paths + gallery_paths]\n",
    "img = np.stack(imgs, axis=0)  # stack the query and gallery images as batch dim\n",
    "output_dict = [\"global_featmaps\", \"featmaps\"]\n",
    "inputs_dict = {'input': img}\n",
    "\n",
    "# result = model(torch.from_numpy(img).to('cuda'))[0].cpu().detach().numpy()\n",
    "result = sess.run(output_dict, inputs_dict)[0]  # only the first output aka global_featmaps is used for REID"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T05:34:39.287016800Z",
     "start_time": "2023-05-29T05:34:39.014016800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(3, 1024)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T05:34:39.302017400Z",
     "start_time": "2023-05-29T05:34:39.287016800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test feature is normalized\n",
      "=> Computing DistMat with euclidean_distance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alien\\Documents\\PyCharm-Projects\\TIL-2023\\CV\\SOLIDER-REID\\utils\\metrics.py:11: UserWarning: This overload of addmm_ is deprecated:\n",
      "\taddmm_(Number beta, Number alpha, Tensor mat1, Tensor mat2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddmm_(Tensor mat1, Tensor mat2, *, Number beta, Number alpha) (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\python_arg_parser.cpp:1485.)\n",
      "  dist_mat.addmm_(1, -2, qf, gf.t())\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('../RT-DETR/dataset/reid/test_old/bounding_box_test/-1_c0s1_1_1.png',\n  0.0016824007),\n ('../RT-DETR/dataset/reid/test_old/bounding_box_test/-1_c0s1_1_2.png',\n  0.002221346)]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.metrics import Postprocessor\n",
    "\n",
    "num_query = len(query_paths)  # each image only has 1 suspect given but if we doing rotation then maybe 4\n",
    "postprocessor = Postprocessor(num_query=num_query, max_rank=50, feat_norm=cfg.TEST.FEAT_NORM, reranking=False)  # in finals cannot use RR as threshold will be changed based on gallery size\n",
    "\n",
    "postprocessor.update(torch.from_numpy(result).to('cuda'))  # postprocessor expects Torch tensor as it uses torch to compute stuff\n",
    "dist_mat = postprocessor.compute()\n",
    "# perform thresholding to determine which gallery image, if any, are matches with the query\n",
    "# dist_mat = (dist_mat < cfg.TEST.THRESHOLD).astype(int)  # boolean array\n",
    "results = []\n",
    "for i, path in enumerate(gallery_paths):\n",
    "    results.append((path, dist_mat[0][i]))\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T05:34:39.347943300Z",
     "start_time": "2023-05-29T05:34:39.304019600Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
