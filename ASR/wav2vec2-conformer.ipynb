{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:04:00.824791800Z",
     "start_time": "2023-05-19T07:03:53.105870300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alc/venvs/til2023/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = 'huggingface'\n",
    "os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = 'True'\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ConformerForCTC, TrainingArguments, Trainer\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Union\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:04:00.838791400Z",
     "start_time": "2023-05-19T07:04:00.821791500Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'facebook/wav2vec2-conformer-rel-pos-large-960h-ft'\n",
    "checkpoint_name = 'checkpoints/checkpoint-750/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:04:01.398692Z",
     "start_time": "2023-05-19T07:04:00.836808200Z"
    }
   },
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 30017/30017 [00:00<00:00, 557352.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset audiofolder/default to /home/alc/TIL-2023/ASR/huggingface/datasets/audiofolder/default-63729925f9f00216/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 30017/30017 [00:00<00:00, 201367.86it/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset audiofolder downloaded and prepared to /home/alc/TIL-2023/ASR/huggingface/datasets/audiofolder/default-63729925f9f00216/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('audiofolder', data_dir='imda_truncated_edited', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:04:13.435825Z",
     "start_time": "2023-05-19T07:04:01.394691100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 3750/3750 [00:00<00:00, 237614.29it/s]\n",
      "Found cached dataset audiofolder (/home/alc/TIL-2023/ASR/huggingface/datasets/audiofolder/default-8b45e18fa8565078/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('audiofolder', data_dir='train', split='train')  # specify split to return a Dataset object instead of a DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:04:13.575387500Z",
     "start_time": "2023-05-19T07:04:13.434826400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:48:01.998601400Z",
     "start_time": "2023-05-19T06:47:53.727631900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': [{'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2534.wav',\n",
       "   'array': array([-0.00189209, -0.00036621, -0.00024414, ...,  0.00109863,\n",
       "           0.00045776, -0.00045776]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2537.wav',\n",
       "   'array': array([ 0.00073242,  0.00094604,  0.00036621, ..., -0.00021362,\n",
       "          -0.00164795, -0.00195312]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2538.wav',\n",
       "   'array': array([ 0.00012207, -0.00018311, -0.00024414, ...,  0.00015259,\n",
       "           0.00045776,  0.0010376 ]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2540.wav',\n",
       "   'array': array([0.0010376 , 0.00088501, 0.00054932, ..., 0.00033569, 0.00088501,\n",
       "          0.00067139]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2545.wav',\n",
       "   'array': array([ 0.003479  ,  0.00344849,  0.00344849, ..., -0.0012207 ,\n",
       "          -0.00109863, -0.0010376 ]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2547.wav',\n",
       "   'array': array([-3.15246582e-02, -3.63464355e-02, -3.45153809e-02, ...,\n",
       "          -1.00708008e-03, -2.13623047e-04,  9.15527344e-05]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2548.wav',\n",
       "   'array': array([ 6.10351562e-05,  1.22070312e-04, -9.15527344e-05, ...,\n",
       "          -1.83105469e-04, -7.01904297e-04,  4.27246094e-04]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2550.wav',\n",
       "   'array': array([-0.00115967, -0.00067139,  0.        , ..., -0.00421143,\n",
       "          -0.01077271, -0.01062012]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2552.wav',\n",
       "   'array': array([-0.00012207,  0.00216675,  0.00152588, ..., -0.02200317,\n",
       "          -0.01800537, -0.01272583]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2554.wav',\n",
       "   'array': array([ 0.03793335,  0.04269409,  0.03256226, ...,  0.00015259,\n",
       "           0.00125122, -0.00485229]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2558.wav',\n",
       "   'array': array([-0.00222778, -0.00186157, -0.00128174, ..., -0.00012207,\n",
       "           0.00054932,  0.00048828]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2565.wav',\n",
       "   'array': array([-0.0211792 , -0.00866699,  0.01547241, ...,  0.00302124,\n",
       "           0.00567627,  0.00634766]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2567.wav',\n",
       "   'array': array([ 0.00042725, -0.00012207, -0.00033569, ...,  0.0005188 ,\n",
       "          -0.00027466, -0.00073242]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2572.wav',\n",
       "   'array': array([ 0.00360107,  0.00289917,  0.00204468, ...,  0.00030518,\n",
       "           0.0015564 , -0.00030518]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2579.wav',\n",
       "   'array': array([ 0.00131226,  0.00170898,  0.00167847, ..., -0.00073242,\n",
       "          -0.00076294, -0.00039673]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-259.wav',\n",
       "   'array': array([-0.02069092, -0.0227356 , -0.02249146, ..., -0.00045776,\n",
       "          -0.00057983, -0.00054932]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-26.wav',\n",
       "   'array': array([-9.15527344e-05,  3.05175781e-05,  1.22070312e-04, ...,\n",
       "           9.15527344e-05,  6.10351562e-05, -9.15527344e-05]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2603.wav',\n",
       "   'array': array([ 3.05175781e-05,  9.15527344e-05, -5.49316406e-04, ...,\n",
       "          -4.24194336e-03, -2.99072266e-03,  9.15527344e-05]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-261.wav',\n",
       "   'array': array([-0.00057983, -0.00057983, -0.00036621, ...,  0.00012207,\n",
       "           0.00030518,  0.        ]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2611.wav',\n",
       "   'array': array([ 1.83105469e-04, -9.15527344e-05, -1.52587891e-04, ...,\n",
       "          -2.34985352e-03, -3.02124023e-03, -3.44848633e-03]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2628.wav',\n",
       "   'array': array([-0.00012207,  0.        ,  0.00039673, ...,  0.00204468,\n",
       "          -0.00302124, -0.0078125 ]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2643.wav',\n",
       "   'array': array([-0.00158691,  0.0015564 ,  0.00186157, ...,  0.00045776,\n",
       "          -0.00195312, -0.0039978 ]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2653.wav',\n",
       "   'array': array([ 0.00012207, -0.00033569, -0.0005188 , ...,  0.00265503,\n",
       "           0.00256348,  0.00244141]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2654.wav',\n",
       "   'array': array([-0.0090332 , -0.00305176, -0.00204468, ..., -0.00704956,\n",
       "          -0.00601196, -0.01019287]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2656.wav',\n",
       "   'array': array([ 0.00027466,  0.00015259,  0.00045776, ..., -0.00018311,\n",
       "          -0.00042725,  0.00085449]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2662.wav',\n",
       "   'array': array([ 2.44140625e-04, -9.15527344e-05,  1.83105469e-04, ...,\n",
       "          -3.66210938e-04, -7.32421875e-04, -1.15966797e-03]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-268.wav',\n",
       "   'array': array([-6.10351562e-05, -7.62939453e-04, -9.46044922e-04, ...,\n",
       "           6.10351562e-03, -1.17492676e-02, -5.46264648e-03]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2682.wav',\n",
       "   'array': array([ 0.00024414, -0.00057983, -0.00045776, ..., -0.00094604,\n",
       "          -0.00061035, -0.00039673]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2683.wav',\n",
       "   'array': array([0.00314331, 0.00302124, 0.00244141, ..., 0.00128174, 0.00149536,\n",
       "          0.00076294]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2694.wav',\n",
       "   'array': array([-2.13623047e-04,  6.10351562e-05,  4.88281250e-04, ...,\n",
       "           1.52587891e-04, -3.90625000e-03,  9.85717773e-03]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2703.wav',\n",
       "   'array': array([-0.00021362, -0.00039673, -0.00027466, ..., -0.00012207,\n",
       "          -0.00073242, -0.00021362]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/alc/TIL-2023/ASR/imda_truncated/audio_part3/3121-2709.wav',\n",
       "   'array': array([ 0.00396729,  0.00402832,  0.00323486, ..., -0.0017395 ,\n",
       "          -0.00167847, -0.00143433]),\n",
       "   'sampling_rate': 16000}],\n",
       " 'annotation': ['MIGHT BECAUSE TRIP PROBABLY JUST BEEN MY TWENTY FIRST TRIP RIGHT SO LONDON AND PARIS AND THEN',\n",
       "  'PROBABLY I MEAN EVEN IT IT WAS SAD THAT I WAS I WAS ALONE',\n",
       "  'SO I HAVE TO QUEUE ALONE BUT I CANT I CANT DENY AND SAY LIKE YOU KNOW IT WASNT',\n",
       "  'PPL ITS SO SAD THAT LIKE IN IN TWO YEARS I WOULD HAVE GONE THRICE',\n",
       "  'BUT I MEAN IM IM NOT RESTRICTING IT TO CALI WE CAN GO SOMEONE ELSE IM JUST SAYING LIKE STATES',\n",
       "  'BUT ANYWAY I THINK HONESTLY BY THAT TIME',\n",
       "  'THAT HAPPEN SHE WILL HAVE MONEY BACK NO SHE WAS PUSHING FOR NEXT YEAR THEN I SAY I GOT NOT NOT ENOUGH MONEY FOR NEXT YEAR EVEN THOUGH I TOLD HER LIKE',\n",
       "  'BUT THEN I CANNOT AFFORD TWO US TRIPS IN A YEAR',\n",
       "  'BECAUSE WE POSTPONE OUR NEW NEWYORK TRIP RIGHT AND THEN I SAID NEXT NEWYEARS WE GO',\n",
       "  'THEN AFTER THAT SHE SAID LIKE OH THEN NEXT YEAR WE NOT GOING THEN IM LIKE NO I CANT DO TWO YEARS TRIP LIKE ONE AND I WAS LIKE',\n",
       "  'I MEAN HONESTLY YOU YOU WANT TO PAY FOR ME THEN OBVIOUSLY SO I GOT LIKE MY LONDON TRIP ALREADY I KNOW LIKE I DIDNT PROMISE YOU EH YOU JUST SAID LIKE',\n",
       "  'IM IM NOT IM NOT LIKE I DONT FEEL BAD HONESTLY',\n",
       "  'WAIT SO WE DONE WITH ACTUALLY ALL THESE WERE DONE WITH RIGHT',\n",
       "  'UNTIL IM DONE WITH EXAMS AND THEN I WILL JUST SIT DOWN AND BINGE',\n",
       "  'B NINE NINE WAITING FOR B NINE NINE TO COME OUT',\n",
       "  'LIKE ONE OF THOSE CULTURAL PLACES YOU KNOW LIKE YOU HAVE HAVE CHINATOWN LITTLEINDIA AND GEYLANG',\n",
       "  'WITH ONE WITH THE ONE WITH LID',\n",
       "  'I NO BUT NO EVEN THE WAY THEY THEY PORTRAYED IT LIKE THERE WAS SOME THERE ARE SOME THERE ARE SOMETHING ABOUT MARVEL TRAILERS LIKE RECENTLY ONE THE RECENT ONES',\n",
       "  'HAWKER CENTERS RIGHT SO THE THREE CULTURAL PLACES',\n",
       "  'EH BUT THATS WHY B NINE NINE IS FUN FOR ME CAUSE LIKE ITS CRIME AND COMEDY',\n",
       "  'A TV NETWORK COMPANY OR A',\n",
       "  'GAME OF THRONES REMEMBER REMEMBER WHEN THE GAME OF THRONE CAME OUT AND THEN ALL OF THEM LIKE',\n",
       "  'OH I KEEP THINKING ABOUT THE COOKIES AND CREAM ONE THAT WAS SO GOOD',\n",
       "  'UH OKAY SO WE COVER THE ONE THE JOBS WE LOOKING AT MINE WOULD BE',\n",
       "  'IN THE FUTURE YA BUT LIKE WHEN I HAVE MONEY',\n",
       "  'I DONT EVEN HAVE THE EQUIPMENT AND I DONT HAVE EXCESS MONEY TO GO AND BUY THE EQUIPMENT THAT MUCH',\n",
       "  'THIS ONE ASKS IF YOU WOULD RATHER HAVE KIDS OR PETS',\n",
       "  'OH WE HAVE A LOT OF THINGS TO SAY BUT WE HAVE TWO HOURS AND SEVENTEEN MINUTES SO ITS FINE',\n",
       "  'I THINK MAYBE ONE OF THE MOST MEMORABLE FOR ME WOULD BE THE FIRST TIME WE WENT TO EAT AT THAI FOOD',\n",
       "  'KEEP PUSHING AWAY I THINK THAT WILL BE ONE',\n",
       "  'THE ORH REMEMBER THAT THAT UM RAMEN JAPANESE PLACE AT PLAZASING THE CORNER ONE',\n",
       "  'THE ONE CHRIS SAID OHMYGOD THAT WAS SO GOOD MINE WAS SO GOOD']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][211*32:212*32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:07:54.055577100Z",
     "start_time": "2023-05-19T07:04:13.576902Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    model_name = 'facebook/wav2vec2-conformer-rel-pos-large-960h-ft'\n",
    "\n",
    "    # separate import for each process\n",
    "    from transformers import Wav2Vec2Processor\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "    batch[\"input_values\"] = [processor(audio[\"array\"], sampling_rate=16000).input_values for audio in batch[\"audio\"]]\n",
    "    batch[\"input_length\"] = [len(b[0]) for b in batch[\"input_values\"]]\n",
    "    batch['length'] = batch[\"input_length\"]\n",
    "    batch[\"labels\"] = processor(text=batch[\"annotation\"]).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "ds = ds.map(prepare_dataset, num_proc=8, batched=True, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[57200, 52592, 33600, 72000, 34400, 61200, 63808, 48592, 46800, 42992]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0:10][\"input_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:08:11.793311800Z",
     "start_time": "2023-05-19T07:08:11.771310700Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(input_features, padding=self.padding, return_tensors=\"pt\")\n",
    "\n",
    "        labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:08:13.616042100Z",
     "start_time": "2023-05-19T07:08:12.258678500Z"
    }
   },
   "outputs": [],
   "source": [
    "wer = evaluate.load(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    return {\"wer\": wer.compute(predictions=pred_str, references=label_str)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:08:18.024710500Z",
     "start_time": "2023-05-19T07:08:13.617039500Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Wav2Vec2ConformerForCTC.from_pretrained(\n",
    "    model_name,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    mask_time_prob=0.7,  # 0.05\n",
    "    mask_time_length=10, # 10\n",
    "    mask_feature_prob=0.7, # 0\n",
    "    mask_feature_length=10, # 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:08:18.036711400Z",
     "start_time": "2023-05-19T07:08:18.022711100Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you run this cell, u freeze the feature encoder\n",
    "# DONT run this cell if you wanna unfreeze the whole model. Right now this gives us better perf.\n",
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:11:13.069961300Z",
     "start_time": "2023-05-19T07:11:13.057404Z"
    }
   },
   "outputs": [],
   "source": [
    "per_gpu_bs = 4\n",
    "effective_bs = 384\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"wav2vec2-checkpoints-imda-bs=384-lr=1e-5\",\n",
    "    overwrite_output_dir =True,\n",
    "    per_device_train_batch_size=per_gpu_bs,\n",
    "    gradient_accumulation_steps=math.ceil(effective_bs/per_gpu_bs),\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=60,\n",
    "    gradient_checkpointing=False,\n",
    "    fp16=True,\n",
    "    # bf16=True,  # for A100\n",
    "    fp16_full_eval=True,\n",
    "    # bf16_full_eval=True,  # for A100\n",
    "    group_by_length=True,  # slows down\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy='epoch',  # epoch\n",
    "    save_safetensors=True,\n",
    "    per_device_eval_batch_size=4,\n",
    "    save_steps=1,\n",
    "    eval_steps=1,\n",
    "    logging_steps=1,\n",
    "    save_total_limit=3,\n",
    "    lr_scheduler_type='cosine',\n",
    "    load_best_model_at_end=True,  # True\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,  # follow fairseq fintuning config\n",
    "\n",
    "    warmup_ratio=0., # check whether IMDA data works first    \n",
    "\n",
    "    # warmup_ratio=0.22, # follow Ranger21\n",
    "    weight_decay=1e-4,  # follow Ranger21\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    report_to=['tensorboard'],\n",
    "    dataloader_num_workers=8 if os.name != 'nt' else 1) # since num threads is 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:08:18.098709400Z",
     "start_time": "2023-05-19T07:08:18.086710100Z"
    }
   },
   "outputs": [],
   "source": [
    "class CTCTrainer(Trainer):\n",
    "    def training_step(self, model: torch.nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        if os.name != 'nt':\n",
    "            accelerator.backward(self.scaler.scale(loss))\n",
    "            # self.scaler.scale(loss).backward()\n",
    "        else:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        return loss.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:08:21.285482Z",
     "start_time": "2023-05-19T07:08:21.272629300Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.name != 'nt':\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator(mixed_precision='fp16', dynamo_backend='eager')  # FP8 needs transformer_engine package which is only on Linux with Hopper GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:08:22.337997500Z",
     "start_time": "2023-05-19T07:08:22.318481400Z"
    }
   },
   "outputs": [],
   "source": [
    "def tri_stage_schedule(epoch: int, max_epoch = training_args.num_train_epochs, stage_ratio = [0.1, 0.4, 0.5], peak_lr = training_args.learning_rate, initial_lr_scale=0.01, final_lr_scale=0.05):\n",
    "    \"\"\"https://github.com/facebookresearch/fairseq/blob/5ecbbf58d6e80b917340bcbf9d7bdbb539f0f92b/fairseq/optim/lr_scheduler/tri_stage_lr_scheduler.py#L51\"\"\"\n",
    "    assert sum(stage_ratio) == 1\n",
    "    current_ratio = epoch / max_epoch\n",
    "    if current_ratio < stage_ratio[0]:  # linear warmup\n",
    "        lrs = torch.linspace(initial_lr_scale * peak_lr, peak_lr, int(stage_ratio[0] * max_epoch))\n",
    "        return lrs[epoch]\n",
    "    elif stage_ratio[0] <= current_ratio <= stage_ratio[1]:  # constant\n",
    "        return peak_lr\n",
    "    else:  # exponential decay\n",
    "        decay_factor = -math.log(final_lr_scale) / (stage_ratio[2] * max_epoch)\n",
    "        return peak_lr * math.exp(-decay_factor * stage_ratio[2] * max_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2023-05-23 21:18:51,278] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:806)\n",
      "   reasons:  ___check_obj_id(self, 139645697584208)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:18:54,906] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:563)\n",
      "   reasons:  ___check_obj_id(self, 139645697005392)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:18:57,479] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:660)\n",
      "   reasons:  ___check_obj_id(self, 139645697013024)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:18:57,501] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:611)\n",
      "   reasons:  ___check_obj_id(self, 139645697010912)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='751' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  751/15000 10:47 < 3:25:15, 1.16 it/s, Epoch 1.00/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   1/1501 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-23 21:18:59,945] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_apply_relative_embeddings' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:741)\n",
      "   reasons:  ___check_obj_id(self, 139645162905776)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:08,788] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '__init__' (<string>:2)\n",
      "   reasons:  tensor 'logits' strides mismatch at index 0. expected 15968, actual 9408\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:09,992] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/activations.py:149)\n",
      "   reasons:  tensor 'input' strides mismatch at index 0. expected 1617920, actual 294912\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:17,902] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/alc/venvs/til2023/lib/python3.10/site-packages/accelerate/utils/operations.py:520)\n",
      "   reasons:  tensor 'kwargs['labels']' strides mismatch at index 0. expected 165, actual 109\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:17,903] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '__call__' (/home/alc/venvs/til2023/lib/python3.10/site-packages/accelerate/utils/operations.py:508)\n",
      "   reasons:  tensor 'kwargs['labels']' strides mismatch at index 0. expected 165, actual 109\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:17,904] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1632)\n",
      "   reasons:  tensor 'labels' strides mismatch at index 0. expected 165, actual 109\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:17,902] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/alc/venvs/til2023/lib/python3.10/site-packages/accelerate/utils/operations.py:520)\n",
      "   reasons:  tensor 'kwargs['labels']' strides mismatch at index 0. expected 165, actual 109\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:17,903] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '__call__' (/home/alc/venvs/til2023/lib/python3.10/site-packages/accelerate/utils/operations.py:508)\n",
      "   reasons:  tensor 'kwargs['labels']' strides mismatch at index 0. expected 165, actual 109\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:17,904] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1632)\n",
      "   reasons:  tensor 'labels' strides mismatch at index 0. expected 165, actual 109\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:18,188] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1315)\n",
      "   reasons:  tensor 'input_values' strides mismatch at index 0. expected 159968, actual 84208\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:18,189] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:506)\n",
      "   reasons:  tensor 'input_values' strides mismatch at index 0. expected 159968, actual 84208\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:20,313] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:311)\n",
      "   reasons:  ___check_obj_id(self, 139645697586464)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:20,428] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_is_fp16_bf16_tensor' (/home/alc/venvs/til2023/lib/python3.10/site-packages/accelerate/utils/operations.py:483)\n",
      "   reasons:  tensor 'tensor' strides mismatch at index 0. expected 15968, actual 8160\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:20,703] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_mask_hidden_states' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1269)\n",
      "   reasons:  tensor 'hidden_states' strides mismatch at index 0. expected 510976, actual 260096\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:20,705] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:456)\n",
      "   reasons:  tensor 'self.pe' dispatch key set mismatch. expected DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:20,729] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '__init__' (<string>:2)\n",
      "   reasons:  tensor 'last_hidden_state' strides mismatch at index 0. expected 510976, actual 260096\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:20,736] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_convert_to_fp32' (/home/alc/venvs/til2023/lib/python3.10/site-packages/accelerate/utils/operations.py:480)\n",
      "   reasons:  tensor 'tensor' strides mismatch at index 0. expected 15968, actual 8128\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:22,161] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/activations.py:77)\n",
      "   reasons:  ___check_obj_id(self, 139645697585264)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:29,107] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_get_feature_vector_attention_mask' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1156)\n",
      "   reasons:  feature_vector_length == 262\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:33,511] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:540)\n",
      "   reasons:  tensor 'hidden_states' strides mismatch at index 0. expected 134144, actual 97792\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:37,361] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_get_feat_extract_output_lengths' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1133)\n",
      "   reasons:  tensor 'input_lengths' strides mismatch at index 0. expected 67600, actual 56400\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:37,361] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_get_feat_extract_output_lengths' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1133)\n",
      "   reasons:  tensor 'input_lengths' strides mismatch at index 0. expected 67600, actual 56400\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-23 21:19:44,739] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_conv_out_length' (/home/alc/venvs/til2023/lib/python3.10/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1142)\n",
      "   reasons:  tensor 'input_length' strides mismatch at index 0. expected 56400, actual 46208\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n"
     ]
    }
   ],
   "source": [
    "# max_steps = math.ceil(training_args.num_train_epochs * len(ds['train']) / training_args.gradient_accumulation_steps / min(training_args.per_device_train_batch_size, len(ds['train'])))\n",
    "# optimizer = Ranger21(model.parameters(), num_iterations=max_steps, lr=1e-4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-8, foreach=False)  # https://github.com/facebookresearch/fairseq/blob/main/examples/wav2vec/config/finetuning/base_960h.yaml\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_steps)\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=tri_stage_schedule)  # following FAIR finetuning settings\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda x: x)  # constant LR, stays same throughout, for Ranger21\n",
    "\n",
    "trainer = CTCTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds['train'],\n",
    "    eval_dataset=ds['test'],\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # optimizers=(optimizer, scheduler),\n",
    ")\n",
    "if os.name != 'nt':  # windows does not support torch.compile yet\n",
    "    # pass\n",
    "    trainer.model_wrapped, trainer.optimizer, trainer.lr_scheduler = accelerator.prepare(trainer.model_wrapped, trainer.optimizer, trainer.lr_scheduler)\n",
    "trainer.train()\n",
    "if os.name != 'nt':\n",
    "    accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T02:59:21.148781100Z",
     "start_time": "2023-05-15T02:59:19.861964700Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.name != 'nt':\n",
    "    trainer.model_wrapped = accelerator.unwrap_model(trainer.model_wrapped)\n",
    "trainer.save_model('wav2vec2-conformer')\n",
    "processor.tokenizer.save_pretrained('wav2vec2-conformer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T10:54:22.540466800Z",
     "start_time": "2023-05-15T10:54:10.025365900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('checkpoints/checkpoint-4125/tokenizer_config.json',\n",
       " 'checkpoints/checkpoint-4125/special_tokens_map.json',\n",
       " 'checkpoints/checkpoint-4125/vocab.json',\n",
       " 'checkpoints/checkpoint-4125/added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "processor = Wav2Vec2Processor.from_pretrained('wav2vec2-conformer')\n",
    "processor.tokenizer.save_pretrained('checkpoints/checkpoint-4125/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T10:52:01.892552800Z",
     "start_time": "2023-05-20T10:51:54.231627400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alc/venvs/til2023/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Infer\n",
    "import os\n",
    "os.environ['HF_HOME'] = 'huggingface'\n",
    "os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = 'True'\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ConformerForCTC\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T10:52:10.801055200Z",
     "start_time": "2023-05-20T10:52:01.894553400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 12000/12000 [00:00<00:00, 25783.84it/s]\n",
      "Found cached dataset audiofolder (/home/alc/TIL-2023/ASR/huggingface/datasets/audiofolder/test-b719a705ed310f32/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"test\", split=\"train\")\n",
    "dataset = KeyDataset(KeyDataset(dataset, \"audio\"), \"array\")\n",
    "test_ds = pd.read_csv('Test_Advanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T10:52:10.814054900Z",
     "start_time": "2023-05-20T10:52:10.799056200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean(annotation):\n",
    "    if \"'\" in annotation:\n",
    "        # print(annotation, f'has \\' in {annotation}, removing')\n",
    "        annotation = annotation.split(\"'\")[0] + annotation.split(\"'\")[1][1:]  # Tokenizer includes \"'\" but TIL dataset does not, remove the S following '\n",
    "    return annotation\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_values = processor(batch, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    return {\"input_values\": input_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-20T10:52:10.817055400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [02:53<00:00,  2.16it/s]\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"wav2vec2-conformer\")\n",
    "data_loader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn, pin_memory=True, num_workers=4 if os.name == 'nt' else 0)\n",
    "checkpoint1 = 'wav2vec2-checkpoints/checkpoint-1593'\n",
    "# checkpoint2 = 'checkpoints/checkpoint-11250'\n",
    "model1 = Wav2Vec2ConformerForCTC.from_pretrained(checkpoint1).to('cuda')\n",
    "# model2 = Wav2Vec2ConformerForCTC.from_pretrained(checkpoint2).to('cuda')\n",
    "model1.eval()\n",
    "# model2.eval()\n",
    "logits1 = []\n",
    "# logits2 = []\n",
    "logits = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(data_loader):\n",
    "        inputs = batch['input_values'].to('cuda')\n",
    "        outputs1 = model1(**inputs).logits\n",
    "        # outputs2 = model2(**inputs).logits\n",
    "        logits1.append(outputs1)\n",
    "        # logits2.append(outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# logits = [(l1 + l2) / 2 for l1, l2 in zip(logits1, logits2)]\n",
    "results = []\n",
    "for l in logits1:\n",
    "    results.extend(processor.batch_decode(torch.argmax(l, dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_ds['annotation'] = list(map(clean,results))\n",
    "test_ds['path'] = test_ds['path'].apply(lambda x: x.split('/')[-1])\n",
    "test_ds.to_csv('Test_Advanced build in aug 0.7 raw 0.2 unfreeze no adam beta val_0.006305.csv', index=False)  # change file name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
