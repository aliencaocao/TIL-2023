{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:04:00.824791800Z",
     "start_time": "2023-05-19T07:03:53.105870300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = 'huggingface'\n",
    "os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = 'True'\n",
    "import math\n",
    "from datasets import Audio, Dataset, DatasetDict, load_dataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ConformerForCTC, TrainingArguments, Trainer\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:04:00.838791400Z",
     "start_time": "2023-05-19T07:04:00.821791500Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name= 'facebook/wav2vec2-conformer-rel-pos-large-960h-ft'\n",
    "checkpoint_name= 'checkpoints/checkpoint-750/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:04:01.398692Z",
     "start_time": "2023-05-19T07:04:00.836808200Z"
    }
   },
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:04:13.435825Z",
     "start_time": "2023-05-19T07:04:01.394691100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 15000/15000 [00:00<00:00, 26192.67it/s] \n",
      "Found cached dataset audiofolder (/home/cheongalc/Documents/til2023/ASR/huggingface/datasets/audiofolder/default-682e93e6f8976099/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('audiofolder', data_dir='audio_augmented_folder', split='train')  # specify split to return a Dataset object instead of a DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:04:13.575387500Z",
     "start_time": "2023-05-19T07:04:13.434826400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:48:01.998601400Z",
     "start_time": "2023-05-19T06:47:53.727631900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/home/cheongalc/Documents/til2023/ASR/audio_augmented_folder/audio_augmented/train_00227_1.1.wav',\n",
       " 'array': array([ 0.00036621,  0.00027466,  0.00027466, ..., -0.01113892,\n",
       "        -0.05032349, -0.04394531]),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']['audio'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:07:54.055577100Z",
     "start_time": "2023-05-19T07:04:13.576902Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8):   0%|          | 0/12000 [00:00<?, ? examples/s]/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:53: FutureWarning: Loading a tokenizer inside Wav2Vec2Processor from a config that does not include a `tokenizer_class` attribute is deprecated and will be removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'` attribute to either your `config.json` or `tokenizer_config.json` file to suppress this warning: \n",
      "  warnings.warn(\n",
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    model_name = 'facebook/wav2vec2-conformer-rel-pos-large-960h-ft'\n",
    "    from transformers import Wav2Vec2Processor\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "    batch[\"input_values\"] = [processor(audio[\"array\"], sampling_rate=16000).input_values for audio in batch[\"audio\"]]\n",
    "    batch[\"input_length\"] = [len(b) for b in batch[\"input_values\"]]\n",
    "    batch['length'] = batch[\"input_length\"]\n",
    "    batch[\"labels\"] = processor(text=batch[\"annotation\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "\n",
    "ds = ds.map(prepare_dataset, num_proc=8, batched=True, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:08:11.793311800Z",
     "start_time": "2023-05-19T07:08:11.771310700Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(input_features, padding=self.padding, return_tensors=\"pt\")\n",
    "\n",
    "        labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:08:13.616042100Z",
     "start_time": "2023-05-19T07:08:12.258678500Z"
    }
   },
   "outputs": [],
   "source": [
    "wer = evaluate.load(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    return {\"wer\": wer.compute(predictions=pred_str, references=label_str)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:08:18.024710500Z",
     "start_time": "2023-05-19T07:08:13.617039500Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Wav2Vec2ConformerForCTC.from_pretrained(\n",
    "    checkpoint_name,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:08:18.036711400Z",
     "start_time": "2023-05-19T07:08:18.022711100Z"
    }
   },
   "outputs": [],
   "source": [
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:11:13.069961300Z",
     "start_time": "2023-05-19T07:11:13.057404Z"
    }
   },
   "outputs": [],
   "source": [
    "per_gpu_bs = 4\n",
    "effective_bs = 32\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints\",\n",
    "    overwrite_output_dir =True,\n",
    "    per_device_train_batch_size=per_gpu_bs,\n",
    "    gradient_accumulation_steps=math.ceil(effective_bs/per_gpu_bs),\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=20,\n",
    "    gradient_checkpointing=False,\n",
    "    fp16=True,\n",
    "    # bf16=True,  # for A100\n",
    "    fp16_full_eval=True,\n",
    "    # bf16_full_eval=True,  # for A100\n",
    "    group_by_length=True,  # slows down\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy='epoch',  # epoch\n",
    "    save_safetensors=True,\n",
    "    per_device_eval_batch_size=4,\n",
    "    save_steps=1,\n",
    "    eval_steps=1,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=3,\n",
    "    lr_scheduler_type='cosine',\n",
    "    load_best_model_at_end=True,  # True\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,  # follow fairseq fintuning config\n",
    "    warmup_ratio=0.22, # follow Ranger21\n",
    "    weight_decay=1e-4,  # follow Ranger21\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    report_to=['tensorboard'],\n",
    "    dataloader_num_workers=24 if os.name != 'nt' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:08:18.098709400Z",
     "start_time": "2023-05-19T07:08:18.086710100Z"
    }
   },
   "outputs": [],
   "source": [
    "class CTCTrainer(Trainer):\n",
    "    def training_step(self, model: torch.nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        if os.name != 'nt':\n",
    "            # accelerator.backward(self.scaler.scale(loss))\n",
    "            self.scaler.scale(loss).backward()\n",
    "        else:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        return loss.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:08:21.285482Z",
     "start_time": "2023-05-19T07:08:21.272629300Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.name != 'nt':\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator(mixed_precision='fp16', dynamo_backend='eager')  # FP8 needs transformer_engine package which is only on Linux with Hopper GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T07:08:22.337997500Z",
     "start_time": "2023-05-19T07:08:22.318481400Z"
    }
   },
   "outputs": [],
   "source": [
    "def tri_stage_schedule(epoch: int, max_epoch = training_args.num_train_epochs, stage_ratio = [0.1, 0.4, 0.5], peak_lr = training_args.learning_rate, initial_lr_scale=0.01, final_lr_scale=0.05):\n",
    "    \"\"\"https://github.com/facebookresearch/fairseq/blob/5ecbbf58d6e80b917340bcbf9d7bdbb539f0f92b/fairseq/optim/lr_scheduler/tri_stage_lr_scheduler.py#L51\"\"\"\n",
    "    assert sum(stage_ratio) == 1\n",
    "    current_ratio = epoch / max_epoch\n",
    "    if current_ratio < stage_ratio[0]:  # linear warmup\n",
    "        lrs = torch.linspace(initial_lr_scale * peak_lr, peak_lr, int(stage_ratio[0] * max_epoch))\n",
    "        return lrs[epoch]\n",
    "    elif stage_ratio[0] <= current_ratio <= stage_ratio[1]:  # constant\n",
    "        return peak_lr\n",
    "    else:  # exponential decay\n",
    "        decay_factor = -math.log(final_lr_scale) / (stage_ratio[2] * max_epoch)\n",
    "        return peak_lr * math.exp(-decay_factor * stage_ratio[2] * max_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2023-05-19 22:52:54,304] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:806)\n",
      "   reasons:  ___check_obj_id(self, 140658442431072)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:01,155] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:563)\n",
      "   reasons:  ___check_obj_id(self, 140658442342800)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:05,741] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:660)\n",
      "   reasons:  ___check_obj_id(self, 140658442343952)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:05,781] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:611)\n",
      "   reasons:  ___check_obj_id(self, 140658442344816)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 4:23:52, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.617200</td>\n",
       "      <td>0.327153</td>\n",
       "      <td>0.134042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.534100</td>\n",
       "      <td>0.293813</td>\n",
       "      <td>0.116633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.424200</td>\n",
       "      <td>0.253052</td>\n",
       "      <td>0.095108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.367200</td>\n",
       "      <td>0.202164</td>\n",
       "      <td>0.081579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.182232</td>\n",
       "      <td>0.069467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.266300</td>\n",
       "      <td>0.147588</td>\n",
       "      <td>0.062314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.237100</td>\n",
       "      <td>0.154062</td>\n",
       "      <td>0.059750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.119504</td>\n",
       "      <td>0.045108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>0.120989</td>\n",
       "      <td>0.041835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.130300</td>\n",
       "      <td>0.114256</td>\n",
       "      <td>0.042713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.091877</td>\n",
       "      <td>0.035560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.100200</td>\n",
       "      <td>0.085830</td>\n",
       "      <td>0.034211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.082728</td>\n",
       "      <td>0.031714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.072552</td>\n",
       "      <td>0.025911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.071345</td>\n",
       "      <td>0.023482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>0.067770</td>\n",
       "      <td>0.022166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.065033</td>\n",
       "      <td>0.020580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.061176</td>\n",
       "      <td>0.020547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.061657</td>\n",
       "      <td>0.020479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.061194</td>\n",
       "      <td>0.020513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-19 22:53:10,242] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_apply_relative_embeddings' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:741)\n",
      "   reasons:  ___check_obj_id(self, 140658442087872)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:28,409] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '__init__' (<string>:2)\n",
      "   reasons:  tensor 'logits' strides mismatch at index 0. expected 8032, actual 9440\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:31,109] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/activations.py:149)\n",
      "   reasons:  tensor 'input' strides mismatch at index 0. expected 786432, actual 296960\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:47,346] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/accelerate/utils/operations.py:520)\n",
      "   reasons:  tensor 'kwargs['labels']' strides mismatch at index 0. expected 63, actual 76\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:47,347] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '__call__' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/accelerate/utils/operations.py:508)\n",
      "   reasons:  tensor 'kwargs['labels']' strides mismatch at index 0. expected 63, actual 76\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:47,349] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1632)\n",
      "   reasons:  tensor 'labels' strides mismatch at index 0. expected 63, actual 76\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:47,346] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/accelerate/utils/operations.py:520)\n",
      "   reasons:  tensor 'kwargs['labels']' strides mismatch at index 0. expected 63, actual 76\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:47,347] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '__call__' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/accelerate/utils/operations.py:508)\n",
      "   reasons:  tensor 'kwargs['labels']' strides mismatch at index 0. expected 63, actual 76\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:47,349] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1632)\n",
      "   reasons:  tensor 'labels' strides mismatch at index 0. expected 63, actual 76\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:47,362] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '<graph break in _compute_mask_indices>' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:152)\n",
      "   reasons:  ___stack0 == 0.3745401188473625\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:51,624] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1315)\n",
      "   reasons:  tensor 'input_values' strides mismatch at index 0. expected 88992, actual 85056\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:55,941] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:456)\n",
      "   reasons:  tensor 'self.pe' dispatch key set mismatch. expected DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:56,016] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_is_fp16_bf16_tensor' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/accelerate/utils/operations.py:483)\n",
      "   reasons:  tensor 'tensor' strides mismatch at index 0. expected 8864, actual 7072\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:57,449] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_mask_hidden_states' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1269)\n",
      "   reasons:  tensor 'hidden_states' strides mismatch at index 0. expected 283648, actual 245760\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:57,450] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_compute_mask_indices' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:116)\n",
      "   reasons:  shape == (4, 277)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:57,515] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '__init__' (<string>:2)\n",
      "   reasons:  tensor 'last_hidden_state' strides mismatch at index 0. expected 283648, actual 245760\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:53:57,521] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_convert_to_fp32' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/accelerate/utils/operations.py:480)\n",
      "   reasons:  tensor 'tensor' strides mismatch at index 0. expected 8864, actual 7680\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:54:19,620] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:506)\n",
      "   reasons:  tensor 'input_values' strides mismatch at index 0. expected 100688, actual 84768\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:54:19,726] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_get_feature_vector_attention_mask' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1156)\n",
      "   reasons:  feature_vector_length == 314\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:54:23,035] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:311)\n",
      "   reasons:  ___check_obj_id(self, 140659039106048)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:54:25,881] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/activations.py:77)\n",
      "   reasons:  ___check_obj_id(self, 140658441976848)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:54:26,193] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:540)\n",
      "   reasons:  tensor 'hidden_states' strides mismatch at index 0. expected 160768, actual 116736\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:54:39,073] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_get_feat_extract_output_lengths' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1133)\n",
      "   reasons:  tensor 'input_lengths' strides mismatch at index 0. expected 84768, actual 67696\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "[2023-05-19 22:54:57,102] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_conv_out_length' (/home/cheongalc/venvs/til2023/lib/python3.9/site-packages/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1142)\n",
      "   reasons:  tensor 'input_length' strides mismatch at index 0. expected 78224, actual 72528\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n"
     ]
    }
   ],
   "source": [
    "# max_steps = math.ceil(training_args.num_train_epochs * len(ds['train']) / training_args.gradient_accumulation_steps / min(training_args.per_device_train_batch_size, len(ds['train'])))\n",
    "# optimizer = Ranger21(model.parameters(), num_iterations=max_steps, lr=1e-4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-8, foreach=False)  # https://github.com/facebookresearch/fairseq/blob/main/examples/wav2vec/config/finetuning/base_960h.yaml\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_steps)\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=tri_stage_schedule)  # following FAIR finetuning settings\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda x: x)  # constant LR, stays same throughout, for Ranger21\n",
    "\n",
    "trainer = CTCTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds['train'],\n",
    "    eval_dataset=ds['test'],\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # optimizers=(optimizer, scheduler),\n",
    ")\n",
    "if os.name != 'nt':  # windows does not support torch.compile yet\n",
    "    # pass\n",
    "    trainer.model_wrapped, trainer.optimizer, trainer.lr_scheduler = accelerator.prepare(trainer.model_wrapped, trainer.optimizer, trainer.lr_scheduler)\n",
    "trainer.train()\n",
    "if os.name != 'nt':\n",
    "    accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T02:59:21.148781100Z",
     "start_time": "2023-05-15T02:59:19.861964700Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.name != 'nt':\n",
    "    trainer.model_wrapped = accelerator.unwrap_model(trainer.model_wrapped)\n",
    "trainer.save_model('wav2vec2-conformer')\n",
    "processor.tokenizer.save_pretrained('wav2vec2-conformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T10:54:22.540466800Z",
     "start_time": "2023-05-15T10:54:10.025365900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('checkpoints/checkpoint-7500/tokenizer_config.json',\n",
       " 'checkpoints/checkpoint-7500/special_tokens_map.json',\n",
       " 'checkpoints/checkpoint-7500/vocab.json',\n",
       " 'checkpoints/checkpoint-7500/added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "processor = Wav2Vec2Processor.from_pretrained('wav2vec2-conformer')\n",
    "processor.tokenizer.save_pretrained('checkpoints/checkpoint-7500/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T10:52:01.892552800Z",
     "start_time": "2023-05-20T10:51:54.231627400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Infer\n",
    "import os\n",
    "os.environ['HF_HOME'] = 'huggingface'\n",
    "os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = 'True'\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ConformerForCTC\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/12000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9d716ecce134fd185fac832fc52d90c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset audiofolder (C:/Users/alien/Documents/PyCharm-Projects/TIL-2023/ASR/huggingface/datasets/audiofolder/test-070480b76e15472b/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"test\", split=\"train\")\n",
    "dataset = KeyDataset(KeyDataset(dataset, \"audio\"), \"array\")\n",
    "test_ds = pd.read_csv('Test_Advanced.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T10:52:10.801055200Z",
     "start_time": "2023-05-20T10:52:01.894553400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def clean(annotation):\n",
    "    if \"'\" in annotation:\n",
    "        # print(annotation, f'has \\' in {annotation}, removing')\n",
    "        annotation = annotation.split(\"'\")[0] + annotation.split(\"'\")[1][1:]  # Tokenizer includes \"'\" but TIL dataset does not, remove the S following '\n",
    "    return annotation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T10:52:10.814054900Z",
     "start_time": "2023-05-20T10:52:10.799056200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\transformers\\modeling_utils.py:433: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  with safe_open(checkpoint_file, framework=\"pt\") as f:\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/375 [00:31<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27166bd6a8f7424d8c76ef81410f6633"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"wav2vec2-conformer\")\n",
    "data_loader = DataLoader(dataset, batch_size=32, collate_fn=processor, pin_memory=True, num_workers=4)\n",
    "checkpoint1 = 'model/checkpoint-2250 aug lb 0.0267'\n",
    "checkpoint2 = 'model/checkpoint-2250 aug lb 0.0267'\n",
    "model1 = Wav2Vec2ConformerForCTC.from_pretrained(checkpoint1).to('cuda')\n",
    "model2 = Wav2Vec2ConformerForCTC.from_pretrained(checkpoint2).to('cuda')\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "logits1 = []\n",
    "logits2 = []\n",
    "logits = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(data_loader):\n",
    "        inputs = processor(batch[\"input_values\"], sampling_rate=16000, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "        outputs1 = model1(**inputs).logits\n",
    "        outputs2 = model2(**inputs).logits\n",
    "        logits1.append(outputs1)\n",
    "        logits2.append(outputs2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-20T10:52:10.817055400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logits = [(l1 + l2) / 2 for l1, l2 in zip(logits1, logits2)]\n",
    "results = []\n",
    "for l in logits:\n",
    "    results.extend(processor.batch_decode(torch.argmax(l, dim=-1)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_ds['annotation'] = list(map(clean, results))\n",
    "test_ds['path'] = test_ds['path'].apply(lambda x: x.split('/')[-1])\n",
    "test_ds.to_csv('Test_Advanced_6750_0.0205.csv', index=False)  # change file name"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
