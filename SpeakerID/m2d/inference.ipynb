{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘weights’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir weights\n",
    "!wget https://github.com/nttcslab/m2d/releases/download/v0.1.0/msm_mae_vit_base-80x608p16x16-220924-mr75.zip \\\n",
    "  -O weights/msm_mae_vit_base-80x608p16x16-220924-mr75.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'weights'\n",
      "/home/alc/TIL-2023/SpeakerID/m2d/weights\n",
      "Archive:  msm_mae_vit_base-80x608p16x16-220924-mr75.zip\n",
      "   creating: msm_mae_vit_base-80x608p16x16-220924-mr75/\n",
      "  inflating: msm_mae_vit_base-80x608p16x16-220924-mr75/checkpoint-300.pth  \n"
     ]
    }
   ],
   "source": [
    "%cd weights\n",
    "!unzip msm_mae_vit_base-80x608p16x16-220924-mr75.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39meval\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mevar.ar_m2d.AR_M2D\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evar' is not defined"
     ]
    }
   ],
   "source": [
    "eval('evar.ar_m2d.AR_M2D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yip/Projects/TIL-2023/SpeakerID/m2d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yip/.local/lib/python3.10/site-packages/nnAudio/Spectrogram.py:4: Warning: importing Spectrogram subpackage will be deprecated soon. You should import the feature extractor from the feature subpackage. See actual documentation.\n",
      "  warnings.warn(\n",
      "ERROR:root:Install toprchopenl3.\n",
      ">>> pip install torchopenl3\n",
      "ERROR:root:Make your copy of VGGish under external folder. Check Preparing-models.md for the details.\n",
      "2023-06-10 16:50:56.510442: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-10 16:51:05.333446: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "ERROR:root:Install tensorflow and tensorflow_hub.\n",
      ">>> pip install tensorflow tensorflow_hub\n",
      "ERROR:root:Make your copy of COALA under external folder. Check Preparing-models.md for the details.\n",
      "ERROR:root:Install transformers.\n",
      ">>> pip install transformers\n",
      "ERROR:root:Install transformers.\n",
      ">>> pip install transformers\n",
      "ERROR:root:Install transformers.\n",
      ">>> pip install transformers\n",
      "ERROR:root:Install transformers.\n",
      ">>> pip install transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+task_metadata=evar/metadata/til.csv,+task_data=work/16k/til,+unit_samples=240000\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/alc/TIL-2023/SpeakerID/m2d/evar/logs/til_ar_m2d.AR_M2D_64f5263c/weights_ep0it0-0.66667_loss0.3365.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/yip/Projects/TIL-2023/SpeakerID/m2d/inference.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yip/Projects/TIL-2023/SpeakerID/m2d/inference.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m cfg\u001b[39m.\u001b[39mruntime_cfg \u001b[39m=\u001b[39m kwarg_cfg(n_class\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, hidden\u001b[39m=\u001b[39m())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yip/Projects/TIL-2023/SpeakerID/m2d/inference.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m run_path \u001b[39m=\u001b[39m Path(\u001b[39m\"\u001b[39m\u001b[39m/home/alc/TIL-2023/SpeakerID/m2d/evar/logs/til_ar_m2d.AR_M2D_64f5263c\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yip/Projects/TIL-2023/SpeakerID/m2d/inference.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(run_path \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mweights_ep0it0-0.66667_loss0.3365.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yip/Projects/TIL-2023/SpeakerID/m2d/inference.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m norm_stats \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(run_path \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnorm_stats.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yip/Projects/TIL-2023/SpeakerID/m2d/inference.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m backbone \u001b[39m=\u001b[39m AR_M2D(cfg, inference_mode\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, norm_stats\u001b[39m=\u001b[39mnorm_stats)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/alc/TIL-2023/SpeakerID/m2d/evar/logs/til_ar_m2d.AR_M2D_64f5263c/weights_ep0it0-0.66667_loss0.3365.pth'"
     ]
    }
   ],
   "source": [
    "%cd ~/Projects/TIL-2023/SpeakerID/m2d\n",
    "import sys\n",
    "sys.path.append(\"./evar\")\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from evar.ar_m2d import AR_M2D\n",
    "from finetune import TaskNetwork\n",
    "from lineareval import make_cfg\n",
    "from evar.common import kwarg_cfg\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "cfg, n_folds, activation, balanced = make_cfg(\n",
    "  config_file=\"evar/config/m2d.yaml\",\n",
    "  task=\"til\",\n",
    "  options=\"\",\n",
    ")\n",
    "cfg.weight_file = \"/home/alc/TIL-2023/SpeakerID/m2d/weights/msm_mae_vit_base-80x608p16x16-220924-mr75/checkpoint-300.pth\"\n",
    "cfg.unit_sec = 15.0\n",
    "# change n_class according to no. of unique speakers\n",
    "cfg.runtime_cfg = kwarg_cfg(n_class=2, hidden=())\n",
    "\n",
    "run_path = Path(\"/home/alc/TIL-2023/SpeakerID/m2d/evar/logs/til_ar_m2d.AR_M2D_64f5263c\")\n",
    "state_dict = torch.load(run_path / \"weights_ep0it0-0.66667_loss0.3365.pth\")\n",
    "norm_stats = torch.load(run_path / \"norm_stats.pt\")\n",
    "\n",
    "backbone = AR_M2D(cfg, inference_mode=True, norm_stats=norm_stats).to(device)\n",
    "model = TaskNetwork(cfg, ar=backbone).to(device)\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "audio = torchaudio.load(\"/home/alc/TIL-2023/SpeakerID/m2d/evar/work/16k/til/valid-2.wav\")[0]\n",
    "classes_pickle_path = \"/home/alc/TIL-2023/SpeakerID/m2d/evar/logs/til_ar_m2d.AR_M2D_64f5263c/classes.pkl\"\n",
    "\n",
    "with open(classes_pickle_path, \"rb\") as classes_pickle_file:\n",
    "  classes = pickle.load(classes_pickle_file)\n",
    "\n",
    "classes = list(classes)\n",
    "\n",
    "model_output = model(audio)\n",
    "pred_idx = torch.argmax(model_output[0])\n",
    "pred_class_name = classes[pred_idx]\n",
    "\n",
    "pred_class_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "til2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
