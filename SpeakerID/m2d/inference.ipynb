{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘weights’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir weights\n",
    "!wget https://github.com/nttcslab/m2d/releases/download/v0.1.0/msm_mae_vit_base-80x608p16x16-220924-mr75.zip \\\n",
    "  -O weights/msm_mae_vit_base-80x608p16x16-220924-mr75.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'weights'\n",
      "/home/alc/TIL-2023/SpeakerID/m2d/weights\n",
      "Archive:  msm_mae_vit_base-80x608p16x16-220924-mr75.zip\n",
      "   creating: msm_mae_vit_base-80x608p16x16-220924-mr75/\n",
      "  inflating: msm_mae_vit_base-80x608p16x16-220924-mr75/checkpoint-300.pth  \n"
     ]
    }
   ],
   "source": [
    "%cd weights\n",
    "!unzip msm_mae_vit_base-80x608p16x16-220924-mr75.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39meval\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mevar.ar_m2d.AR_M2D\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evar' is not defined"
     ]
    }
   ],
   "source": [
    "eval('evar.ar_m2d.AR_M2D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alc/TIL-2023/SpeakerID/m2d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alc/venvs/til2023/lib/python3.10/site-packages/nnAudio/Spectrogram.py:4: Warning: importing Spectrogram subpackage will be deprecated soon. You should import the feature extractor from the feature subpackage. See actual documentation.\n",
      "  warnings.warn(\n",
      "/home/alc/venvs/til2023/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ERROR:root:Install toprchopenl3.\n",
      ">>> pip install torchopenl3\n",
      "ERROR:root:Make your copy of VGGish under external folder. Check Preparing-models.md for the details.\n",
      "ERROR:root:Install tensorflow and tensorflow_hub.\n",
      ">>> pip install tensorflow tensorflow_hub\n",
      "ERROR:root:Make your copy of COALA under external folder. Check Preparing-models.md for the details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+task_metadata=evar/metadata/til.csv,+task_data=work/16k/til,+unit_samples=240000\n",
      "MSM-MAE **FORCED DEC DEPTH AS 4 (Your decoder_depth=4 is ignored)**\n",
      "MaskedAutoencoderViT(in_chans=1, patch size=(16, 16), grid_size=[ 5 38],\n",
      "  embed_dim=768, depth=12, num_heads=12, decoder_embed_dim=384,\n",
      "  decoder_depth=4, decoder_num_heads=6, mlp_ratio=4,\n",
      "  norm_pix_loss=False)\n",
      " using 206 parameters, while dropped 0 from 206 parameters in /home/alc/TIL-2023/SpeakerID/m2d/weights/msm_mae_vit_base-80x608p16x16-220924-mr75/checkpoint-300.pth\n",
      "3840 [] 2\n",
      "Backbone representation:\n",
      "Total number of parameters: 92,967,040 (trainable 92,747,008)\n",
      "Trainable parameters: ['runtime.backbone.cls_token', 'runtime.backbone.mask_token', 'runtime.backbone.patch_embed.proj.weight', 'runtime.backbone.patch_embed.proj.bias', 'runtime.backbone.blocks.0.norm1.weight', 'runtime.backbone.blocks.0.norm1.bias', 'runtime.backbone.blocks.0.attn.qkv.weight', 'runtime.backbone.blocks.0.attn.qkv.bias', 'runtime.backbone.blocks.0.attn.proj.weight', 'runtime.backbone.blocks.0.attn.proj.bias'] ...\n",
      "Others are frozen such as: ['runtime.backbone.pos_embed', 'runtime.backbone.decoder_pos_embed'] \n",
      "Head:\n",
      "Total number of parameters: 7,682 (trainable 7,682)\n",
      "Trainable parameters: ['mlp.mlp.0.weight', 'mlp.mlp.0.bias']\n",
      "Others are frozen such as: [] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): TaskNetwork(\n",
       "    (ar): AR_M2D(\n",
       "      (runtime): RuntimeM2D(\n",
       "        (backbone): MaskedAutoencoderViT(\n",
       "          (patch_embed): PatchEmbed(\n",
       "            (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "            (norm): Identity()\n",
       "          )\n",
       "          (blocks): ModuleList(\n",
       "            (0-11): 12 x Block(\n",
       "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (decoder_embed): Linear(in_features=768, out_features=384, bias=True)\n",
       "          (decoder_blocks): ModuleList(\n",
       "            (0-3): 4 x Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (decoder_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (decoder_pred): Linear(in_features=384, out_features=256, bias=True)\n",
       "        )\n",
       "        (to_spec): MelSpectrogram(\n",
       "          Mel filter banks size = (80, 201), trainable_mel=False\n",
       "          (stft): STFT(n_fft=400, Fourier Kernel size=(201, 1, 400), iSTFT=False, trainable=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): TaskHead(\n",
       "      (norm): BatchNorm1d(3840, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (mlp): MLP(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=3840, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ~/TIL-2023/SpeakerID/m2d\n",
    "import sys\n",
    "sys.path.append(\"./evar\")\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from evar.ar_m2d import AR_M2D\n",
    "from finetune import TaskNetwork\n",
    "from lineareval import make_cfg\n",
    "from evar.common import kwarg_cfg\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "cfg, n_folds, activation, balanced = make_cfg(\n",
    "  config_file=\"evar/config/m2d.yaml\",\n",
    "  task=\"til\",\n",
    "  options=\"\",\n",
    ")\n",
    "cfg.weight_file = \"/home/alc/TIL-2023/SpeakerID/m2d/weights/msm_mae_vit_base-80x608p16x16-220924-mr75/checkpoint-300.pth\"\n",
    "cfg.unit_sec = 15.0\n",
    "# change n_class according to no. of unique speakers\n",
    "cfg.runtime_cfg = kwarg_cfg(n_class=2, hidden=())\n",
    "\n",
    "run_path = Path(\"/home/alc/TIL-2023/SpeakerID/m2d/evar/logs/til_ar_m2d.AR_M2D_64f5263c\")\n",
    "state_dict = torch.load(run_path / \"weights_ep0it0-0.66667_loss0.3365.pth\")\n",
    "norm_stats = torch.load(run_path / \"norm_stats.pt\")\n",
    "\n",
    "backbone = AR_M2D(cfg, inference_mode=True, norm_stats=norm_stats).to(device)\n",
    "model = TaskNetwork(cfg, ar=backbone).to(device)\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "audio = torchaudio.load(\"/home/alc/TIL-2023/SpeakerID/m2d/evar/work/16k/til/valid-2.wav\")[0]\n",
    "classes_pickle_path = \"/home/alc/TIL-2023/SpeakerID/m2d/evar/logs/til_ar_m2d.AR_M2D_64f5263c/classes.pkl\"\n",
    "\n",
    "with open(classes_pickle_path, \"rb\") as classes_pickle_file:\n",
    "  classes = pickle.load(classes_pickle_file)\n",
    "\n",
    "classes = list(classes)\n",
    "\n",
    "model_output = model(audio)\n",
    "pred_idx = torch.argmax(model_output[0])\n",
    "pred_class_name = classes[pred_idx]\n",
    "\n",
    "pred_class_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "til2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
